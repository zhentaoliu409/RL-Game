# RL-Game
This project implements two 3D platformer modes using Q-learning. Mode 1 ("Pluck Stars") requires jumping toward star-shaped obstacles for scoring, while Mode 2 ("Just Jump") demands precise leaps over dynamically generated cubes.

## Table of Contents
- [Game Design](#game-design)
- [Q-Learning Algorithm](#q-Learning-algorithm)
- [Game Interaction](#game-interaction)
- [Installation and Usage](#installation-and-usage)
- [Performance Evaluation](#performance-evaluation)
- [Results](#results)
- [Notice](#notice)
- [References](#references)

## Game Design
Mode 1 (Pluck Stars): Control the ball to jump to the star-shaped obstacles and collect points by jumping.  

Mode 2 (Just Jump): Adaptive strategy, jump or avoid dynamically generated obstacles (small/large squares),  
score points by jumping over small obstacles.  
  
  
### Environment:  
Both game modes use similar base environments and are set to simulate a (top-down view) 3D jumping game:  

1. On a straight road, the player (ball) is always at z=0, and obstacles are constantly generated from a distance
and approaching the player's ball (in this way simulating the behaviour of the ball's constant advancement and avoiding
the creation of a complex state design)

2. The player makes four kinds of decisions based on the information reflected in the screen (distance to the next
obstacle): left/right/jump/don’t move (automatic advance), so that the player's ball scores points by jumping to
the star-shaped obstacle (Mode 1) / jumping over the small rectangular obstacle (Mode 2)  
  
  
### State Space:
- **Mode 1 (Pluck Stars)**: [ x_distance, z_distance ]  
- **Mode 2 (Just Jump)**: [ x_distance,  z_distance,  obstacle_type (Large/Small) ]  

### Action Space:  
- **Action**: [ -1: left,  0: none,  1: right,  2: jump ]

### Reward Function:  
**Mode 1 (Pluck Stars)**:
- Correct movement (+5),   
- Successful jump and pick star (+100),  
- Degree of height matching(+0 ~ 10),  
- Negative: wrong movement (-5),  
- No star picking (-50)
    
**Mode 2 (Just Jump)**:
- Correct movement (+10/5),
- Valid jump (+100)
- Negative: wrong movement (-10/5), 
- Useless jump (-50),
- Collision (-50)  


## Q-Learning Algorithm  
Q-learning is a model-free RL algorithm that iteratively updates Q-values (Q(s,a)) to learn optimal policies.  
Q-learning learns optimal policies by maintaining a Q-table where each entry Q(s,a) represents the expected cumulative  
reward for taking action a in state s.  

### Why it fits this game
- Finite discrete states: States are obstacle positions (x_distance, z_distance) and types (Mode 2 only).  
- Explicit action space: Actions {-1, 0, 1, 2} (left, stay, right, jump) are discrete and tabularly manageable.  
- Sparse interpretable rewards: Key events (jump success: +100, collision: -50) are tractable for Q-value updates.  
- Real-time Decision: Action selection via table lookup (arg maxQ(s,a)) meets 60FPS real-time requirements.

### Policy Effectiveness
**Prioritized Experience Replay**: Prioritised experience replay is the sorting of experiences by TD error  
(|r + γ maxQ(s′) − Q(s,a)|) and priority replay of high error samples.

**Dynamic ε-Decay**:
- Design: Initial ε=1.0 (full random), decayed by 0.995 per episode (min ε=0.01).
- Early phase: High ε explores diverse actions (e.g., random jump or horizontal movement) to discover potentially highly rewarding paths.
- Late phase: Low ε utilises learned policies (e.g. jumping at the right place and distance) to steadily improve scores.

## Game Interaction  
### Modular architecture  
The system has been designed with modularity in mind, allowing separate processes for the AI agent, the user/AI interface and the game  
environment. This ensures that each component is independent, allowing for modification and further design.  


### Interface design  
  

- menu and subprocess Interface: The MainMenu class provides a user-friendly interface for selecting game modes, and the
co-existence of the user/AI interface makes it easier to compare the user's level of play with that of the AI agent.

- Real-time status display: The user interface displays the current state of the game environment, including the player's
position and any obstacles. This is updated in real time using Pygame's rendering capabilities.

- Action and real-time feedback: User actions (keystrokes) and AI actions (generated by the QLearningAgent) are immediately
reflected in the game environment. The GUI displays the agent's actions and the resulting rewards or penalties.


## Installation and Usage

### Installation

To set up the environment and run this project, follow these steps:

1. Clone the repository:
   ```bash
   git clone https://github.com/zhentaoliu409/RL-Jump_Game.git
   ```

2. Install the required libraries:
   ```bash
   pip install pygame gzip pickle
   ```

